{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_1feature_LSTM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeLUB1BJ57aq"
      },
      "source": [
        "import csv\r\n",
        "import sys\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import itertools\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt_NO1C1wgzQ"
      },
      "source": [
        "path = '/content/drive/My Drive/AmericanPics/Test_traces/'\r\n",
        "all_files = glob.glob(os.path.join(path, \"ssdtrace_expanded_FULL.csv0_deathtime_added.csv\"))\r\n",
        "f = all_files[0]  # Change the file name as required\r\n",
        "print(\"File \" + str(f))\r\n",
        "df = pd.read_csv(f,skiprows =2,header=None,na_values=['-1'], index_col=False,low_memory=False)\r\n",
        "cols = ['IO','device_major','device_minor','cpu_id','timestamp','io_sequence','process_id','default_action','lba','unknown','IO_size','deathtime']\r\n",
        "df.columns = cols\r\n",
        "lba_list = df['lba'].tolist()\r\n",
        "deathtime_list = df['deathtime'].tolist()\r\n",
        "print(\"Min LBA in the dataset :\", min(lba_list))\r\n",
        "print(\"Max LBA in the dataset :\", max(lba_list))\r\n",
        "print(\"Number of IO Accesses :\",len(df))\r\n",
        "print(\"Number of unique LBAs : \",len(Counter(lba_list)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZz0CXtouhPM"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8zbTkhCnDyz"
      },
      "source": [
        "# Drop unnecessary coloumns\n",
        "\n",
        "print(\"Before drop: {}\".format(df.columns))\n",
        "df.drop(df.columns[[0,1,2,3,5,6,7,9]], axis=1,inplace=True) \n",
        "print(\"After drop: {}\".format(df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY6Rii4KkaNe"
      },
      "source": [
        "df = df.dropna()\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PEW_GwwuwPp"
      },
      "source": [
        "df.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsxnVKIEedaT"
      },
      "source": [
        "# print(\"Before Drop\", len(df))\n",
        "# df = df.dropna()\n",
        "# df = df.reset_index(drop=True)\n",
        "# df = df[df.deathtime == -1.00]\n",
        "# df = df[df.deathtime == 0.00]\n",
        "# print(\"After Drop\", len(df))\n",
        "print(len(Counter(df['deathtime'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hydqtQWqrKr6"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmQdkXGLDJXm"
      },
      "source": [
        "|df = df.reset_index(drop=True)\n",
        "df = df.sort_values(by='timestamp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-jfqRTdzD4i"
      },
      "source": [
        "num_classes = 10\n",
        "interval = 100/num_classes\n",
        "deathtime_list = df['deathtime'].tolist()\n",
        "deathtime_range_list = []\n",
        "for x in range(num_classes):\n",
        "  deathtime_range_list.append(np.percentile(deathtime_list, (x+1)*interval))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmRCIB_xzH3h"
      },
      "source": [
        "counter = 0\n",
        "deathtime_class = []\n",
        "while(counter < len(deathtime_list)):\n",
        "  dt = deathtime_list[counter]\n",
        "  dt_class = min(deathtime_range_list, key=lambda x:abs(x-dt))\n",
        "  deathtime_class.append(dt_class)\n",
        "  counter = counter + 1\n",
        "print(len(deathtime_class))\n",
        "print(len(deathtime_list))\n",
        "df['dt_class'] = deathtime_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJh3Apz6uq5B"
      },
      "source": [
        "counter = 0\n",
        "last_n_digits = 5\n",
        "lba_list = df['lba'].tolist()\n",
        "lba_high = []\n",
        "lba_low = []\n",
        "\n",
        "while(counter < len(lba_list)):\n",
        "  lba = lba_list[counter]\n",
        "  high = str(lba)[:-last_n_digits]\n",
        "  low = str(lba)[-last_n_digits:]\n",
        "  lba_high.append(int(high))\n",
        "  lba_low.append(int(low))\n",
        "  counter = counter + 1\n",
        "print(len(lba_high))\n",
        "print(len(lba_low))\n",
        "df['lba_high'] = lba_high\n",
        "df['lba_low'] = lba_low"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wP0ucivwUFr"
      },
      "source": [
        "# Normalize lba, deathtime and response time\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "cols_to_normalize = ['deathtime']\n",
        "for column in cols_to_normalize:\n",
        "  new_column = column + '_norm'\n",
        "  df[new_column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())     \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA6K89PJD130"
      },
      "source": [
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tAocsjc-xu6"
      },
      "source": [
        "a = df['dt_class'].unique().tolist()\n",
        "operation_id_map = {}\n",
        "for i,id in enumerate(a): operation_id_map[id] = i \n",
        "df['Output_Class'] = df['dt_class'].map(lambda x: operation_id_map[x])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns9CQORr64pN"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRhF7pz_JE9v"
      },
      "source": [
        "# # Drop unnecessary coloumns\n",
        "\n",
        "# print(\"Before drop: {}\".format(df.columns))\n",
        "# df.drop(df.columns[[0,1,2,3,4,5,7,8]], axis=1,inplace=True) \n",
        "# print(\"After drop: {}\".format(df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ssZnuqPDOB"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGUvT7abpOwy"
      },
      "source": [
        "# Split to train, validate and test\n",
        "# Finding the value 75th percentile of TimeStamp\n",
        "import math\n",
        "\n",
        "training_pt_1 = math.floor((len(df)*0.75)) \n",
        "\n",
        "# lba_train =df[:training_pt_1]['lba_norm'].tolist()\n",
        "# lba_test = df[training_pt_1+1:]['lba_norm'].tolist()\n",
        "\n",
        "# response_norm_train = df[:training_pt_1]['response_norm'].tolist()\n",
        "# response_norm_test = df[training_pt_1+1:]['response_norm'].tolist()\n",
        "\n",
        "death_time_norm_train = df[:training_pt_1]['deathtime_norm'].tolist()\n",
        "death_time_norm_test = df[training_pt_1+1:]['deathtime_norm'].tolist()\n",
        "\n",
        "# lba_low_train = df[:training_pt_1]['lba_low_norm'].tolist()\n",
        "# lba_low_test = df[training_pt_1+1:]['lba_low_norm'].tolist()\n",
        "\n",
        "# lba_high_train = df[:training_pt_1]['lba_high_norm'].tolist()\n",
        "# lba_high_test =  df[training_pt_1+1:]['lba_high_norm'].tolist()\n",
        "\n",
        "# size_norm_train = df[:training_pt_1]['IO_size'].tolist()\n",
        "# size_norm_test = df[training_pt_1+1:]['IO_size'].tolist()\n",
        "\n",
        "output_train = df[:training_pt_1]['Output_Class'].tolist()\n",
        "output_test = df[training_pt_1+1:]['Output_Class'].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrn7r6Z6KQDd"
      },
      "source": [
        "# lba_train= np.array(lba_train).reshape(-1,1)\n",
        "# lba_test= np.array(lba_test).reshape(-1,1)\n",
        "\n",
        "# response_norm_train = np.array(response_norm_train).reshape(-1,1)\n",
        "# response_norm_test = np.array(response_norm_test).reshape(-1,1)\n",
        "\n",
        "death_time_norm_train= np.array(death_time_norm_train).reshape(-1,1)\n",
        "death_time_norm_test= np.array(death_time_norm_test).reshape(-1,1)\n",
        "\n",
        "# lba_low_train= np.array(lba_low_train).reshape(-1,1)\n",
        "# lba_low_test= np.array(lba_low_test).reshape(-1,1)\n",
        "\n",
        "# lba_high_train= np.array(lba_high_train).reshape(-1,1)\n",
        "# lba_high_test= np.array(lba_high_test).reshape(-1,1)\n",
        "\n",
        "# size_train= np.array(lba_low_train).reshape(-1,1)\n",
        "# size_test= np.array(lba_low_test).reshape(-1,1)\n",
        "\n",
        "output_train= np.array(output_train).reshape(-1,1)\n",
        "output_test= np.array(output_test).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62_cpZkgKyjy"
      },
      "source": [
        "def create_dataset2(dataset, window_size):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset) - 2 * window_size):\n",
        "        a = dataset[i:(i + window_size), 0]\n",
        "        #print(a)\n",
        "        dataX.append(a)\n",
        "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
        "        #print(b)\n",
        "        dataY.append(b)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "lstm_num_timesteps = 32\n",
        "    \n",
        "# X_train_lba, y_train_lba = create_dataset2(lba_train, lstm_num_timesteps)\n",
        "# X_test_lba, y_test_lba = create_dataset2(lba_test, lstm_num_timesteps)\n",
        "\n",
        "# X_train_response, y_train_response = create_dataset2(response_norm_train, lstm_num_timesteps)\n",
        "# X_test_response, y_test_response = create_dataset2(response_norm_test, lstm_num_timesteps)\n",
        "\n",
        "X_train_deathtime, y_train_deathtime = create_dataset2(death_time_norm_train, lstm_num_timesteps)\n",
        "X_test_deathtime, y_test_deathtime = create_dataset2(death_time_norm_test, lstm_num_timesteps)\n",
        "\n",
        "# X_train_lba_low, y_train_lba_low = create_dataset2(lba_low_train, lstm_num_timesteps)\n",
        "# X_test_lba_low, y_test_lba_low = create_dataset2(lba_low_test, lstm_num_timesteps)\n",
        "\n",
        "# X_train_lba_high, y_train_lba_high = create_dataset2(lba_high_train, lstm_num_timesteps)\n",
        "# X_test_lba_high, y_test_lba_high = create_dataset2(lba_high_test, lstm_num_timesteps)\n",
        "\n",
        "# X_size, y_train_size = create_dataset2(size_train, lstm_num_timesteps)\n",
        "# X_size, y_test_size = create_dataset2(size_test, lstm_num_timesteps)\n",
        "\n",
        "X_train_output, y_train_output = create_dataset2(output_train, lstm_num_timesteps)\n",
        "X_test_output, y_test_output = create_dataset2(output_test, lstm_num_timesteps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlO62kF9y8OV"
      },
      "source": [
        "lstm_num_features = 1\n",
        "lstm_predict_sequences = True\n",
        "lstm_num_predictions = 32\n",
        "\n",
        "# y_train_lba = np.reshape(y_train_lba, (y_train_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "# y_test_lba = np.reshape(y_test_lba, (y_test_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "\n",
        "y_train_deathtime = np.reshape(y_train_deathtime, (y_train_deathtime.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "y_test_deathtime = np.reshape(y_test_deathtime, (y_test_deathtime.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "\n",
        "# y_train_lba_high = np.reshape(y_train_lba_high, (y_train_lba_high.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "# y_test_lba_high = np.reshape(y_test_lba_high, (y_test_lba_high.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "\n",
        "# y_train_response = np.reshape(y_train_response, (y_train_response.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "# y_test_response = np.reshape(y_test_response, (y_test_response.shape[0], lstm_num_predictions, lstm_num_features))                        \n",
        "\n",
        "# y_train_lba_low = np.reshape(y_train_lba_low, (y_train_lba_low.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "# y_test_lba_low = np.reshape(y_test_lba_low, (y_test_lba_low.shape[0], lstm_num_predictions, lstm_num_features))                        \n",
        "\n",
        "# y_train_size = np.reshape(y_train_size, (y_train_size.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "# y_test_size = np.reshape(y_test_size, (y_test_size.shape[0], lstm_num_predictions, lstm_num_features)) \n",
        "\n",
        "y_train_output = np.reshape(y_train_output, (y_train_output.shape[0], lstm_num_predictions, lstm_num_features))\n",
        "y_test_output = np.reshape(y_test_output, (y_test_output.shape[0], lstm_num_predictions, lstm_num_features))                        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXHwGzms4qTG"
      },
      "source": [
        "from math import sqrt\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import EarlyStopping,TensorBoard\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate , Dot\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, Reshape\n",
        "\n",
        "maxlen= 32\n",
        "# vocabulary_1 = len(Counter(df['lba_norm']))\n",
        "# vocabulary_2 = len(Counter(df['IO_size']))\n",
        "vocabulary_3 = len(Counter(df['deathtime_norm']))\n",
        "# vocabulary_4 = len(Counter(df['lba_low_norm']))\n",
        "# vocabulary_5 = len(Counter(df['lba_high_norm']))\n",
        "vocabulary_6 = len(Counter(df['Output_Class']))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hidden_size = 250\n",
        "\n",
        "# input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
        "# inputA=Input(shape=(maxlen,),dtype='float64')  \n",
        "# inputB=Input(shape=(maxlen,),dtype='float64') \n",
        "inputC=Input(shape=(maxlen,),dtype='float64') \n",
        "# inputD=Input(shape=(maxlen,),dtype='float64')\n",
        "# inputE=Input(shape=(maxlen,),dtype='float64') \n",
        "inputF=Input(shape=(maxlen,),dtype='float64')  \n",
        "\n",
        "\n",
        "# # the first branch operates on the first input\n",
        "# x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
        "# x = Model(inputs=inputA, outputs=x)\n",
        "\n",
        "# # # the second branch opreates on the second input\n",
        "# y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
        "# y = Model(inputs=inputB, outputs=y)\n",
        "\n",
        "# # the third branch opreates on the third input\n",
        "z = Embedding(input_dim=vocabulary_3,output_dim=hidden_size,input_length=maxlen)(inputC)\n",
        "z = Model(inputs=inputC, outputs=z)\n",
        "\n",
        "# # # the fourth branch opreates on the third input\n",
        "# w = Embedding(input_dim=vocabulary_4,output_dim=hidden_size,input_length=maxlen)(inputD)\n",
        "# w = Model(inputs=inputD, outputs=w)\n",
        "\n",
        "# # # the fifth branch opreates on the third input\n",
        "# u = Embedding(input_dim=vocabulary_5,output_dim=hidden_size,input_length=maxlen)(inputE)\n",
        "# u = Model(inputs=inputE, outputs=u)\n",
        "\n",
        "# # the sixth branch opreates on the third input\n",
        "v = Embedding(input_dim=vocabulary_6,output_dim=hidden_size,input_length=maxlen)(inputF)\n",
        "v = Model(inputs=inputF, outputs=v)\n",
        "\n",
        "# combine the output of the two branches\n",
        "combined = keras.layers.concatenate([z.output,v.output])\n",
        "\n",
        "lstm1 = TCN(hidden_size,return_sequences=True)(combined)\n",
        "lstm2 = TCN(hidden_size, return_sequences=True)(lstm1)\n",
        "\n",
        "# create classification output\n",
        "output = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_6, activation='softmax'), name='output')(lstm2)\n",
        "\n",
        "model =Model([inputC,inputF],[output]) # combining all into a Keras model\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss={'output': 'sparse_categorical_crossentropy'},\n",
        "              metrics={ 'output': 'categorical_accuracy'})\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91PruT_3L0U"
      },
      "source": [
        "import time\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
        "\n",
        "\n",
        "print('Train...')\n",
        "start_time = time.time()\n",
        "valid = [y_test_deathtime,y_test_output],[y_test_output]\n",
        "\n",
        "model.fit([X_train_deathtime,X_train_output],[y_train_output],\n",
        "          verbose=1,epochs=75,batch_size = batch_size,callbacks=[monitor,checkpointer],validation_data=valid)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qDpzOsUijZq"
      },
      "source": [
        "pred1 = model.predict([(y_test_deathtime,y_test_output)],verbose =1 )\n",
        "pred_1 = pred1[:,i,:]\n",
        "pred1 = np.argmax(pred_1, axis=1)\n",
        "Counter(pred1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJPJ0QOWpOJi"
      },
      "source": [
        "counter = 0\n",
        "correct = 0\n",
        "while(counter < len(pred1)):\n",
        "  og_label = output_test[counter]\n",
        "  model_op = pred1[counter]\n",
        "  counter = counter + 1\n",
        "  if (og_label == model_op):\n",
        "       correct = correct + 1\n",
        "print(\"Accuracy\",correct/len(pred1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1kLFkFtAqq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}