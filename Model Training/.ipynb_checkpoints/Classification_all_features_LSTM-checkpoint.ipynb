{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeLUB1BJ57aq"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt_NO1C1wgzQ"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/AmericanPics/Test_traces/'\n",
    "all_files = glob.glob(os.path.join(path, \"ssdtrace_expanded_FULL.csv0_deathtime_added.csv\"))\n",
    "f = all_files[0]  # Change the file name as required\n",
    "print(\"File \" + str(f))\n",
    "df = pd.read_csv(f,skiprows =2,header=None,na_values=['-1'], index_col=False,low_memory=False)\n",
    "cols = ['IO','device_major','device_minor','cpu_id','timestamp','io_sequence','process_id','default_action','lba','unknown','IO_size','deathtime']\n",
    "df.columns = cols\n",
    "lba_list = df['lba'].tolist()\n",
    "deathtime_list = df['deathtime'].tolist()\n",
    "print(\"Min LBA in the dataset :\", min(lba_list))\n",
    "print(\"Max LBA in the dataset :\", max(lba_list))\n",
    "print(\"Number of IO Accesses :\",len(df))\n",
    "print(\"Number of unique LBAs : \",len(Counter(lba_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZz0CXtouhPM"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8zbTkhCnDyz"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary coloumns\n",
    "\n",
    "print(\"Before drop: {}\".format(df.columns))\n",
    "df.drop(df.columns[[0,1,2,3,5,6,7,9]], axis=1,inplace=True) \n",
    "print(\"After drop: {}\".format(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY6Rii4KkaNe"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PEW_GwwuwPp"
   },
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsxnVKIEedaT"
   },
   "outputs": [],
   "source": [
    "# print(\"Before Drop\", len(df))\n",
    "# df = df.dropna()\n",
    "# df = df.reset_index(drop=True)\n",
    "# df = df[df.deathtime == -1.00]\n",
    "# df = df[df.deathtime == 0.00]\n",
    "# print(\"After Drop\", len(df))\n",
    "print(len(Counter(df['deathtime'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hydqtQWqrKr6"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmQdkXGLDJXm"
   },
   "outputs": [],
   "source": [
    "|df = df.reset_index(drop=True)\n",
    "df = df.sort_values(by='timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-jfqRTdzD4i"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "interval = 100/num_classes\n",
    "deathtime_list = df['deathtime'].tolist()\n",
    "deathtime_range_list = []\n",
    "for x in range(num_classes):\n",
    "  deathtime_range_list.append(np.percentile(deathtime_list, (x+1)*interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmRCIB_xzH3h"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "deathtime_class = []\n",
    "while(counter < len(deathtime_list)):\n",
    "  dt = deathtime_list[counter]\n",
    "  dt_class = min(deathtime_range_list, key=lambda x:abs(x-dt))\n",
    "  deathtime_class.append(dt_class)\n",
    "  counter = counter + 1\n",
    "print(len(deathtime_class))\n",
    "print(len(deathtime_list))\n",
    "df['dt_class'] = deathtime_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJh3Apz6uq5B"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "last_n_digits = 5\n",
    "lba_list = df['lba'].tolist()\n",
    "lba_high = []\n",
    "lba_low = []\n",
    "\n",
    "while(counter < len(lba_list)):\n",
    "  lba = lba_list[counter]\n",
    "  high = str(lba)[:-last_n_digits]\n",
    "  low = str(lba)[-last_n_digits:]\n",
    "  lba_high.append(int(high))\n",
    "  lba_low.append(int(low))\n",
    "  counter = counter + 1\n",
    "print(len(lba_high))\n",
    "print(len(lba_low))\n",
    "df['lba_high'] = lba_high\n",
    "df['lba_low'] = lba_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wP0ucivwUFr"
   },
   "outputs": [],
   "source": [
    "# Normalize lba, deathtime and response time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "cols_to_normalize = ['deathtime']\n",
    "for column in cols_to_normalize:\n",
    "  new_column = column + '_norm'\n",
    "  df[new_column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA6K89PJD130"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tAocsjc-xu6"
   },
   "outputs": [],
   "source": [
    "a = df['dt_class'].unique().tolist()\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): operation_id_map[id] = i \n",
    "df['Output_Class'] = df['dt_class'].map(lambda x: operation_id_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns9CQORr64pN"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGUvT7abpOwy"
   },
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "import math\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['lba_norm'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['lba_norm'].tolist()\n",
    "\n",
    "death_time_norm_train = df[:training_pt_1]['deathtime_norm'].tolist()\n",
    "death_time_norm_test = df[training_pt_1+1:]['deathtime_norm'].tolist()\n",
    "\n",
    "lba_low_train = df[:training_pt_1]['lba_low_norm'].tolist()\n",
    "lba_low_test = df[training_pt_1+1:]['lba_low_norm'].tolist()\n",
    "\n",
    "lba_high_train = df[:training_pt_1]['lba_high_norm'].tolist()\n",
    "lba_high_test =  df[training_pt_1+1:]['lba_high_norm'].tolist()\n",
    "\n",
    "size_norm_train = df[:training_pt_1]['IO_size'].tolist()\n",
    "size_norm_test = df[training_pt_1+1:]['IO_size'].tolist()\n",
    "\n",
    "output_train = df[:training_pt_1]['Output_Class'].tolist()\n",
    "output_test = df[training_pt_1+1:]['Output_Class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wrn7r6Z6KQDd"
   },
   "outputs": [],
   "source": [
    "lba_train= np.array(lba_train).reshape(-1,1)\n",
    "lba_test= np.array(lba_test).reshape(-1,1)\n",
    "\n",
    "death_time_norm_train= np.array(death_time_norm_train).reshape(-1,1)\n",
    "death_time_norm_test= np.array(death_time_norm_test).reshape(-1,1)\n",
    "\n",
    "lba_low_train= np.array(lba_low_train).reshape(-1,1)\n",
    "lba_low_test= np.array(lba_low_test).reshape(-1,1)\n",
    "\n",
    "lba_high_train= np.array(lba_high_train).reshape(-1,1)\n",
    "lba_high_test= np.array(lba_high_test).reshape(-1,1)\n",
    "\n",
    "size_train= np.array(lba_low_train).reshape(-1,1)\n",
    "size_test= np.array(lba_low_test).reshape(-1,1)\n",
    "\n",
    "output_train= np.array(output_train).reshape(-1,1)\n",
    "output_test= np.array(output_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62_cpZkgKyjy"
   },
   "outputs": [],
   "source": [
    "def create_dataset2(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - 2 * window_size):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        #print(a)\n",
    "        dataX.append(a)\n",
    "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
    "        #print(b)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "lstm_num_timesteps = 32\n",
    "    \n",
    "X_train_lba, y_train_lba = create_dataset2(lba_train, lstm_num_timesteps)\n",
    "X_test_lba, y_test_lba = create_dataset2(lba_test, lstm_num_timesteps)\n",
    "\n",
    "\n",
    "X_train_deathtime, y_train_deathtime = create_dataset2(death_time_norm_train, lstm_num_timesteps)\n",
    "X_test_deathtime, y_test_deathtime = create_dataset2(death_time_norm_test, lstm_num_timesteps)\n",
    "\n",
    "X_train_lba_low, y_train_lba_low = create_dataset2(lba_low_train, lstm_num_timesteps)\n",
    "X_test_lba_low, y_test_lba_low = create_dataset2(lba_low_test, lstm_num_timesteps)\n",
    "\n",
    "X_train_lba_high, y_train_lba_high = create_dataset2(lba_high_train, lstm_num_timesteps)\n",
    "X_test_lba_high, y_test_lba_high = create_dataset2(lba_high_test, lstm_num_timesteps)\n",
    "\n",
    "X_size, y_train_size = create_dataset2(size_train, lstm_num_timesteps)\n",
    "X_size, y_test_size = create_dataset2(size_test, lstm_num_timesteps)\n",
    "\n",
    "X_train_output, y_train_output = create_dataset2(output_train, lstm_num_timesteps)\n",
    "X_test_output, y_test_output = create_dataset2(output_test, lstm_num_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlO62kF9y8OV"
   },
   "outputs": [],
   "source": [
    "lstm_num_features = 1\n",
    "lstm_predict_sequences = True\n",
    "lstm_num_predictions = 64\n",
    "\n",
    "y_train_lba = np.reshape(y_train_lba, (y_train_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba = np.reshape(y_test_lba, (y_test_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "\n",
    "y_train_deathtime = np.reshape(y_train_deathtime, (y_train_deathtime.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_deathtime = np.reshape(y_test_deathtime, (y_test_deathtime.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "\n",
    "y_train_lba_high = np.reshape(y_train_lba_high, (y_train_lba_high.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba_high = np.reshape(y_test_lba_high, (y_test_lba_high.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "\n",
    "y_train_response = np.reshape(y_train_response, (y_train_response.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_response = np.reshape(y_test_response, (y_test_response.shape[0], lstm_num_predictions, lstm_num_features))                        \n",
    "\n",
    "y_train_lba_low = np.reshape(y_train_lba_low, (y_train_lba_low.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba_low = np.reshape(y_test_lba_low, (y_test_lba_low.shape[0], lstm_num_predictions, lstm_num_features))                        \n",
    "\n",
    "y_train_size = np.reshape(y_train_size, (y_train_size.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_size = np.reshape(y_test_size, (y_test_size.shape[0], lstm_num_predictions, lstm_num_features)) \n",
    "\n",
    "y_train_output = np.reshape(y_train_output, (y_train_output.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_output = np.reshape(y_test_output, (y_test_output.shape[0], lstm_num_predictions, lstm_num_features))                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXHwGzms4qTG"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping,TensorBoard\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate , Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, Reshape\n",
    "\n",
    "maxlen= 64\n",
    "vocabulary_1 = len(Counter(df['lba_norm']))\n",
    "vocabulary_2 = len(Counter(df['IO_size']))\n",
    "vocabulary_3 = len(Counter(df['deathtime_norm']))\n",
    "vocabulary_4 = len(Counter(df['lba_low_norm']))\n",
    "vocabulary_5 = len(Counter(df['lba_high_norm']))\n",
    "vocabulary_6 = len(Counter(df['Output_Class']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_size = 500\n",
    "\n",
    "input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "inputA=Input(shape=(maxlen,),dtype='float64')  \n",
    "inputB=Input(shape=(maxlen,),dtype='float64') \n",
    "inputC=Input(shape=(maxlen,),dtype='float64') \n",
    "inputD=Input(shape=(maxlen,),dtype='float64')\n",
    "inputE=Input(shape=(maxlen,),dtype='float64') \n",
    "inputF=Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# # the third branch opreates on the third input\n",
    "z = Embedding(input_dim=vocabulary_3,output_dim=hidden_size,input_length=maxlen)(inputC)\n",
    "z = Model(inputs=inputC, outputs=z)\n",
    "\n",
    "# # the fourth branch opreates on the third input\n",
    "w = Embedding(input_dim=vocabulary_4,output_dim=hidden_size,input_length=maxlen)(inputD)\n",
    "w = Model(inputs=inputD, outputs=w)\n",
    "\n",
    "# # the fifth branch opreates on the third input\n",
    "u = Embedding(input_dim=vocabulary_5,output_dim=hidden_size,input_length=maxlen)(inputE)\n",
    "u = Model(inputs=inputE, outputs=u)\n",
    "\n",
    "# # the sixth branch opreates on the third input\n",
    "v = Embedding(input_dim=vocabulary_6,output_dim=hidden_size,input_length=maxlen)(inputF)\n",
    "v = Model(inputs=inputF, outputs=v)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = keras.layers.concatenate([z.output,v.output])\n",
    "\n",
    "lstm1 = LSTM(hidden_size,return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "# create classification output\n",
    "output = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_6, activation='softmax'), name='output')(lstm2)\n",
    "\n",
    "model =Model([inputC,inputF],[output]) # combining all into a Keras model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'output': 'sparse_categorical_crossentropy'},\n",
    "              metrics={ 'output': 'categorical_accuracy'})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a91PruT_3L0U"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=1, save_best_only=True) # save best model\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "valid = [y_test_lba,y_test_size,y_train_lba_low,y_test_lba_high,y_test_deathtime,y_test_output],[y_test_output]\n",
    "\n",
    "model.fit([X_train_lba,X_train_size,X_train_lba_low,X_train_lba_high,X_train_deathtime,X_train_output],[y_train_output],\n",
    "          verbose=1,epochs=75,batch_size = batch_size,callbacks=[monitor,checkpointer],validation_data=valid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qDpzOsUijZq"
   },
   "outputs": [],
   "source": [
    "pred1 = model.predict([(y_test_lba,y_test_size,y_train_lba_low,y_test_lba_high,y_test_deathtime,y_test_output)],verbose =1 )\n",
    "pred_1 = pred1[:,i,:]\n",
    "pred1 = np.argmax(pred_1, axis=1)\n",
    "Counter(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJPJ0QOWpOJi"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "correct = 0\n",
    "while(counter < len(pred1)):\n",
    "  og_label = output_test[counter]\n",
    "  model_op = pred1[counter]\n",
    "  counter = counter + 1\n",
    "  if (og_label == model_op):\n",
    "       correct = correct + 1\n",
    "print(\"Accuracy\",correct/len(pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C1kLFkFtAqq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Classification_all_features_LSTM.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
